---
title: TensorFlow解决 MNIST数字识别问题一
categories:
  - TensorFlow

  
tags:
  - TensorFlow
abbrlink: 33760
date: 2019-08-10 01:29:56
---
## 模型训练注释写的比较清楚
```python
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# MNIST数据集相关常数
INPUT_NODE = 784
# 0-9
OUTPUT_NODE = 10
# 配置神经网络参数 隐藏层一个500个节点
LAYER1_NODE = 500
# 一个训练batch中的训练数据个数
BATCH_SIZE = 100
# 基础学习率
LEARNING_RATE_BASE = 0.8
# 学习率的衰减率
LEARNING_RATE_DECAY = 0.99
# 描述模型复杂度的正则化项在损失函数中的系数
REGULARIZATION_RATE = 0.0001
# 训练轮数
TRAINING_STEPS = 20000
# 滑动平均衰减率
MOVING_AVERAGE_DECAY = 0.99
'''
一个辅助函数 给定神经网络的输入和所有参数 计算神经网络的前向传播结果
在这里定义了一个使用relu激活函数的三层全连接神经网络，通过加入隐含层实现了多层网络结构
通过relu激活函数实现了去线性化，在这个函数中也支持传入用于计算参数平均值的类 这样方便在测试时使用滑动平均模型
'''


def inference(input_tensor, avg_class, weights1, biases1, weight2, biases2):
    # 没有提供滑动平均类时，直接使用参数当前的取值
    if avg_class == None:
        # 计算隐藏层的前向传播结果，这里使用了relu激活函数
        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)
        # 计算输出层的前置结果 因为在计算损失函数时会一并计算softmax函数
        # 所以这里不需要加入激活函数 而且不加入softmax不会影响预测结果。因为预测时使用的不同类别对应节点输出值的相对大小
        # 没有softmax层对最后分类结果的计算没有影响 于是在计算整个神经网络的前向传播时可以不加入最后的softmax
        return tf.matmul(layer1, weight2) + biases2

    else:
        # 首先使用avg_class.average函数计算出变量的平均滑动值
        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))
        return tf.matmul(layer1, avg_class.average(weight2)) + avg_class.average(biases2)


# 训练模型过程
def train(mnist):
    x = tf.placeholder(tf.float32, shape=[None, INPUT_NODE], name='x-input')
    y_ = tf.placeholder(tf.float32, shape=[None, OUTPUT_NODE], name='y-input')
    # 生成隐藏层参数
    # truncated_normal 产生截断正态分布随机数，取值范围为 [ mean - 2 * stddev, mean + 2 * stddev ]
    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))
    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))
    # 生成输出层参数
    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))
    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))
    # 函数不会使用参数的滑动平均值
    y = inference(x, None, weights1, biases1, weights2, biases2)
    # 定义存储训练轮数的变量，不需要计算滑动平均值，不可训练的变量 trainable=False
    # 使用Tensorflow训练神经网络时 一般会将代表训练轮数的变量指定为不可训练的变量
    global_step = tf.Variable(0, trainable=False)
    # 给定滑动平均衰减率和训练轮数的变量，
    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
    # 在所有代表神经网络参数的变量上使用滑动平均
    variable_averages_op = variable_averages.apply(tf.trainable_variables())
    # 计算使用了互动平均之后的前向传播结果 第四章中介绍过滑动平均不会改变变量本身的取值 而是维护一个影子
    # 变量来记录其滑动平均值。所以当需要使用这个平滑平均值时 需要明确调用average函数
    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)
    '''
     计算交叉熵作为刻画预测值和真实值之间差距的损失函数，这里使用了Tensorflow中提供的sparse_softmax_cross_entroy_with_l
     计算交叉熵。当分类问题只有一个正确的数字时 可以使用这个函数加速交叉熵的计算
     MNIST问题的图片中只包含了0-9个数字，所以可以使用这个函数计算交叉熵损失 第一个参数是神经网络不包括softmax层
     的前向传播结果，第二个事训练数据的正确答案 因为标准答案是一个长度为10的一维数组 而该函数需要提供的是一个正确答案的数字
     所以需要使用tf.argmax函数来得到正确答案所对应的类别编号
    '''
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))
    # 计算当前batch中所有样例的交叉熵平均值
    cross_entropy_mean = tf.reduce_mean(cross_entropy)
    # 计算L2正则化损失函数
    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)
    # 计算模型的正则化损失，一般只计算神经网络边上权重的正则化损失，而不用偏置项
    regularization = regularizer(weights1) + regularizer(weights2)
    # 总损失等于交叉熵损失和正则化损失的和
    loss = cross_entropy_mean + regularization
    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,  # 基础的学习率 随着叠代的进行 更新变量时使用 学习率在这个基础上递减
                                               global_step,  # 当前叠代的轮数
                                               mnist.train.num_examples,  # batch size 过完所有训练需要的迭代次数
                                               LEARNING_RATE_DECAY)  # 学习率衰减速度
    # 使用 tf.train.GradientDescentOptimizer 优化算法来优化损失函数 损失函数包含了交叉熵损失和L2正则化损失
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)
    # 训练神经网络时 每过一遍数据即需要通过反向传播更新神经网络的参数
    # 又要更新每一个参数的滑动平均值 为了一次完成多个操作 TensorFlow提供了 tf.control_dependencies 和tf.group两种机制
    # 下面两行程序是等价的  train_op=tf.group(train_step,variables_averages_op)
    with tf.control_dependencies([train_step, variable_averages_op]):
        train_op = tf.no_op(name='train')
    # 检验使用了滑动平均模型的网络向前传播是否正确，tf,argmax(average_y,1)
    # 计算每一个样例的预测答案 其中 average_y是一个batch_size*10的二维数组 每个样例代表一个前向传播的结果
    # tf.argmax第二个参数1表示选取最大值的操作尽在第一个维度中进行，也就是说，只在每一行选取最大值对应的下标
    # 于是得到的结果时一个长度为batch的一维数组 这个一维数组中的值就代表了每一个样例对应的数据识别结果
    # tf.equal判断两个张量的每一维是否相等相等为true
    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))
    # 这个运算首先将一个布尔数组转化为实数值然后计算平均。这个平均值就是模型在一组数据上的正确率
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    saver = tf.train.Saver()
    with tf.Session() as sess:
        tf.global_variables_initializer().run()
        variable_feed = {x: mnist.validation.images, y_: mnist.validation.labels}
        test_feed = {x: mnist.test.images, y_: mnist.test.labels}
        for i in range(TRAINING_STEPS):
            if (i % 1000 == 0):
                # 计算滑动平均模型在验证数据上的结果 太大的batch回导致计算时间过长发生内存溢出的错误
                variable_acc = sess.run(accuracy, feed_dict=variable_feed)
                print("After %d training step(s),validation accuracy "
                      "using average model is %g" % (i, variable_acc))
            # 这一轮使用的batch的训练数据并运行训练结果
            xs, ys = mnist.train.next_batch(BATCH_SIZE)
            sess.run(train_op, feed_dict={x: xs, y_: ys})

        # 在训练结束后，在测试数据上检测神经网络的最终正确率
        test_acc = sess.run(accuracy, feed_dict=test_feed)
        print("After %d training step(s),test accuracy using average"
              "model is %g" % (TRAINING_STEPS, test_acc))
       
def loadModel():
    saver=tf.train.import_meta_graph("G:\\workspace-python\\python_way\\tf\\mnist\\data\\mmnist_model.ckpt.meta")
    with tf.Session() as sess:
        saver.restore(sess,"G:\\workspace-python\\python_way\\tf\\mnist\\data\\mmnist_model.ckpt")
def main(argv=None):
    # 声明处理MNIST数据集的类，在这个类初始化时会自动下载数据
    mnist = input_data.read_data_sets("G:\\workspace-python\\python_way\\tf\\mnist\\data", one_hot=True)
    # mnist.test.labels
    # mnist.train.num_examples
    train(mnist)


if __name__ == '__main__':
    tf.app.run()

```

## TensorFlow 训练过程及结果 经过20000此叠代后准确率98.4% 后面会用卷积准确率回更高
```
Extracting G:\workspace-python\python_way\tf\mnist\data\train-images-idx3-ubyte.gz
Extracting G:\workspace-python\python_way\tf\mnist\data\train-labels-idx1-ubyte.gz
Extracting G:\workspace-python\python_way\tf\mnist\data\t10k-images-idx3-ubyte.gz
Extracting G:\workspace-python\python_way\tf\mnist\data\t10k-labels-idx1-ubyte.gz
2019-12-06 14:07:23.325145: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2019-12-06 14:07:23.325145: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2019-12-06 14:07:23.325145: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2019-12-06 14:07:23.326145: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2019-12-06 14:07:23.326145: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2019-12-06 14:07:23.326145: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
After 0 training step(s),validation accuracy using average model is 0.0938
After 1000 training step(s),validation accuracy using average model is 0.9752
After 2000 training step(s),validation accuracy using average model is 0.9816
After 3000 training step(s),validation accuracy using average model is 0.983
After 4000 training step(s),validation accuracy using average model is 0.985
After 5000 training step(s),validation accuracy using average model is 0.9842
After 6000 training step(s),validation accuracy using average model is 0.9844
After 7000 training step(s),validation accuracy using average model is 0.9844
After 8000 training step(s),validation accuracy using average model is 0.9848
After 9000 training step(s),validation accuracy using average model is 0.9846
After 10000 training step(s),validation accuracy using average model is 0.9846
After 11000 training step(s),validation accuracy using average model is 0.9858
After 12000 training step(s),validation accuracy using average model is 0.9842
After 13000 training step(s),validation accuracy using average model is 0.985
After 14000 training step(s),validation accuracy using average model is 0.9858
After 15000 training step(s),validation accuracy using average model is 0.986
After 16000 training step(s),validation accuracy using average model is 0.9856
After 17000 training step(s),validation accuracy using average model is 0.985
After 18000 training step(s),validation accuracy using average model is 0.986
After 19000 training step(s),validation accuracy using average model is 0.9856
After 20000 training step(s),test accuracy using averagemodel is 0.9846
```
## 保存模型
```
    saver = tf.train.Saver()
            # 保存模型
            saver.save(sess, "G:\\workspace-python\\python_way\\tf\\mnist\\data\\mmnist_model.ckpt")
```
## 模型加载
```python
    saver=tf.train.import_meta_graph("G:\\workspace-python\\python_way\\tf\\mnist\\data\\mmnist_model.ckpt.meta")
    with tf.Session() as sess:
        saver.restore(sess,"G:\\workspace-python\\python_way\\tf\\mnist\\data\\mmnist_model.ckpt")
```
